{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Computer_Vision_Pre_Lab_7.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GeYAT5q6qKS1",
        "0dfX4doetGgl",
        "___Gly7aswIS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xb59fUU1S3u"
      },
      "source": [
        "# Student Details\n",
        "* Guy Kabiri\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7qHmuurugnX"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql-LkORnrgbd"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "from torch.optim.optimizer import Optimizer, required"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNrpeKCzuiSA"
      },
      "source": [
        "# Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeYAT5q6qKS1"
      },
      "source": [
        "## Adam\n",
        "Adam is a stochastic gradient-based optimizer that is used to train deep learning models.  \n",
        "It is based on lower-order moments of the gradients. Because it is based on first-order derivatives, all the parameters are in the same computational complexity and little memory.\n",
        "Its name `Adam` is derived from adaptive movements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htA0tUUPqE33"
      },
      "source": [
        "class Adam(Optimizer):\n",
        "    \"\"\"Implements Adam algorithm.\n",
        "\n",
        "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
        "    The implementation of the L2 penalty follows changes proposed in\n",
        "    `Decoupled Weight Decay Regularization`_.\n",
        "\n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
        "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
        "            (default: False)\n",
        "\n",
        "    .. _Adam\\: A Method for Stochastic Optimization:\n",
        "        https://arxiv.org/abs/1412.6980\n",
        "    .. _Decoupled Weight Decay Regularization:\n",
        "        https://arxiv.org/abs/1711.05101\n",
        "    .. _On the Convergence of Adam and Beyond:\n",
        "        https://openreview.net/forum?id=ryQu7f-RZ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, amsgrad=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
        "        super(Adam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Adam, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            params_with_grad = []\n",
        "            grads = []\n",
        "            exp_avgs = []\n",
        "            exp_avg_sqs = []\n",
        "            state_sums = []\n",
        "            max_exp_avg_sqs = []\n",
        "            state_steps = []\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    params_with_grad.append(p)\n",
        "                    if p.grad.is_sparse:\n",
        "                        raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                    grads.append(p.grad)\n",
        "\n",
        "                    state = self.state[p]\n",
        "                    # Lazy state initialization\n",
        "                    if len(state) == 0:\n",
        "                        state['step'] = 0\n",
        "                        # Exponential moving average of gradient values\n",
        "                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                        # Exponential moving average of squared gradient values\n",
        "                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                        if group['amsgrad']:\n",
        "                            # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                            state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                    exp_avgs.append(state['exp_avg'])\n",
        "                    exp_avg_sqs.append(state['exp_avg_sq'])\n",
        "\n",
        "                    if group['amsgrad']:\n",
        "                        max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
        "\n",
        "                    # update the steps for each param group update\n",
        "                    state['step'] += 1\n",
        "                    # record the step after step update\n",
        "                    state_steps.append(state['step'])\n",
        "\n",
        "            beta1, beta2 = group['betas']\n",
        "            F.adam(params_with_grad,\n",
        "                   grads,\n",
        "                   exp_avgs,\n",
        "                   exp_avg_sqs,\n",
        "                   max_exp_avg_sqs,\n",
        "                   state_steps,\n",
        "                   group['amsgrad'],\n",
        "                   beta1,\n",
        "                   beta2,\n",
        "                   group['lr'],\n",
        "                   group['weight_decay'],\n",
        "                   group['eps'])\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dfX4doetGgl"
      },
      "source": [
        "## SGD\n",
        "Stochastic gradient descent is an iterative method for optimizing. This method replaces the actual gradient with an estimated theory, which in high dimensional problems, reduces the computational complexity.  \n",
        "Instead of calculating the loss for the whole set, it calculated each time for a small random set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HuwR7ltqE15"
      },
      "source": [
        "class SGD(Optimizer):\n",
        "    \"\"\"Implements stochastic gradient descent (optionally with momentum).\n",
        "\n",
        "    Nesterov momentum is based on the formula from\n",
        "    `On the importance of initialization and momentum in deep learning`__.\n",
        "\n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float): learning rate\n",
        "        momentum (float, optional): momentum factor (default: 0)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        dampening (float, optional): dampening for momentum (default: 0)\n",
        "        nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
        "\n",
        "    Example:\n",
        "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "        >>> optimizer.zero_grad()\n",
        "        >>> loss_fn(model(input), target).backward()\n",
        "        >>> optimizer.step()\n",
        "\n",
        "    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
        "\n",
        "    .. note::\n",
        "        The implementation of SGD with Momentum/Nesterov subtly differs from\n",
        "        Sutskever et. al. and implementations in some other frameworks.\n",
        "\n",
        "        Considering the specific case of Momentum, the update can be written as\n",
        "\n",
        "        .. math::\n",
        "            \\begin{aligned}\n",
        "                v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n",
        "                p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n",
        "            \\end{aligned}\n",
        "\n",
        "        where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the \n",
        "        parameters, gradient, velocity, and momentum respectively.\n",
        "\n",
        "        This is in contrast to Sutskever et. al. and\n",
        "        other frameworks which employ an update of the form\n",
        "\n",
        "        .. math::\n",
        "            \\begin{aligned}\n",
        "                v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n",
        "                p_{t+1} & = p_{t} - v_{t+1}.\n",
        "            \\end{aligned}\n",
        "\n",
        "        The Nesterov version is analogously modified.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
        "                 weight_decay=0, nesterov=False):\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
        "                        weight_decay=weight_decay, nesterov=nesterov)\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        super(SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            params_with_grad = []\n",
        "            d_p_list = []\n",
        "            momentum_buffer_list = []\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "            lr = group['lr']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    params_with_grad.append(p)\n",
        "                    d_p_list.append(p.grad)\n",
        "\n",
        "                    state = self.state[p]\n",
        "                    if 'momentum_buffer' not in state:\n",
        "                        momentum_buffer_list.append(None)\n",
        "                    else:\n",
        "                        momentum_buffer_list.append(state['momentum_buffer'])\n",
        "\n",
        "            F.sgd(params_with_grad,\n",
        "                  d_p_list,\n",
        "                  momentum_buffer_list,\n",
        "                  weight_decay,\n",
        "                  momentum,\n",
        "                  lr,\n",
        "                  dampening,\n",
        "                  nesterov)\n",
        "\n",
        "            # update momentum_buffers in state\n",
        "            for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
        "                state = self.state[p]\n",
        "                state['momentum_buffer'] = momentum_buffer\n",
        "\n",
        "        return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "___Gly7aswIS"
      },
      "source": [
        "## AdamW\n",
        "Similar to the `Adam` optimizer, but while the `Adam` optimizer implements `R2` regularization, `AdamW` is a weighted decay regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsonRpe1syvl"
      },
      "source": [
        "class AdamW(Optimizer):\n",
        "    \"\"\"Implements AdamW algorithm.\n",
        "\n",
        "    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.\n",
        "    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.\n",
        "\n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n",
        "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
        "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
        "            (default: False)\n",
        "\n",
        "    .. _Adam\\: A Method for Stochastic Optimization:\n",
        "        https://arxiv.org/abs/1412.6980\n",
        "    .. _Decoupled Weight Decay Regularization:\n",
        "        https://arxiv.org/abs/1711.05101\n",
        "    .. _On the Convergence of Adam and Beyond:\n",
        "        https://openreview.net/forum?id=ryQu7f-RZ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=1e-2, amsgrad=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
        "        super(AdamW, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(AdamW, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            params_with_grad = []\n",
        "            grads = []\n",
        "            exp_avgs = []\n",
        "            exp_avg_sqs = []\n",
        "            state_sums = []\n",
        "            max_exp_avg_sqs = []\n",
        "            state_steps = []\n",
        "            amsgrad = group['amsgrad']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                params_with_grad.append(p)\n",
        "                if p.grad.is_sparse:\n",
        "                    raise RuntimeError('AdamW does not support sparse gradients')\n",
        "                grads.append(p.grad)\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                    if amsgrad:\n",
        "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                        state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                exp_avgs.append(state['exp_avg'])\n",
        "                exp_avg_sqs.append(state['exp_avg_sq'])\n",
        "\n",
        "                if amsgrad:\n",
        "                    max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
        "\n",
        "                beta1, beta2 = group['betas']\n",
        "                # update the steps for each param group update\n",
        "                state['step'] += 1\n",
        "                # record the step after step update\n",
        "                state_steps.append(state['step'])\n",
        "\n",
        "            F.adamw(params_with_grad,\n",
        "                    grads,\n",
        "                    exp_avgs,\n",
        "                    exp_avg_sqs,\n",
        "                    max_exp_avg_sqs,\n",
        "                    state_steps,\n",
        "                    amsgrad,\n",
        "                    beta1,\n",
        "                    beta2,\n",
        "                    group['lr'],\n",
        "                    group['weight_decay'],\n",
        "                    group['eps'])\n",
        "\n",
        "        return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj_EaixF1apO"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1ACzfhO1kWr"
      },
      "source": [
        "## Binary Cross Entropy with logits loss (BCEwithLogitsLoss)\n",
        "\n",
        "> $ BCE = -\\sum_{i=1}^{C'=2}t_{i} log (s_{i}) = -t_{1} log(s_{1}) - (1 - t_{1}) log(1 - s_{1}) $  \n",
        "\n",
        "Binary Cross-Entropy loss is used when assumed that there are only two classes.  \n",
        "It is a sigmoid activation layer with a cross-entropy loss after it. It is different from a softmax loss in that each output vector is independent, unlike softmax which each vector depends on the others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqyqZKxV1o0d"
      },
      "source": [
        "## Cross-Entropy\n",
        "\n",
        "> $ CE = -log\\left ( \\frac{e^{s_{p}}}{\\sum_{j}^{C} e^{s_{j}}} \\right ) $  \n",
        "\n",
        "Cross-Entropy loss measures the difference between probability distributions for a set of events.  \n",
        "It takes the probabilities of the events and calculates the distance from the actual values.  \n",
        "`Entropy` comes from the information theory which measured the number of bits required to transmit a randomly selected event from a probability distribution.  \n",
        "`Cross-Entropy` builds upon the idea of entropy from information theory and calculates the number of bits required to represent or transmit an average event from one distribution compared to another distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hatAwj5DSpb"
      },
      "source": [
        "## Focal Loss\n",
        "\n",
        "> $ FL = -\\sum_{i=1}^{n}(1 - s_{i})^{\\gamma }t_{i} log (s_{i}) $\n",
        "\n",
        "Focal loss is an improved version of `Cross-Entropy`, that addresses class imbalance during training in object detection by assigning more weights to hard misclassified examples.  \n",
        "It reduces the contribution to the loss from easy examples and increases the importance of hard examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX_oNS4tDWpy"
      },
      "source": [
        "def focal_loss(p, alpha=0.25, gama=2):\n",
        "    return - alpha * np.pow(1 - p, gama) * np.log(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBHzyXbc1tzT"
      },
      "source": [
        "## Dice\n",
        "\n",
        "> $ FL = \\sum_{i=1}^{n} 1 - \\frac{2  y  p_{i}}{y + p_{i} + 1} $  \n",
        "\n",
        "\n",
        "Dice loss is a better alternative to `Cross-Emtropy` for boundary detection tasks in computer vision.  \n",
        "It is a similarity loss function, which means, it calculates the similarity between two samples."
      ]
    }
  ]
}
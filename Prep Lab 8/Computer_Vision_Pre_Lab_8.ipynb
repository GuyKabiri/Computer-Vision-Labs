{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Computer_Vision_Pre_Lab_8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsEvz1tQPzaY"
      },
      "source": [
        "# Preparatory Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SHQVpIuPKrN"
      },
      "source": [
        "## Student Details:\n",
        "* Guy Kabiri 312252224"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4V0I_prP0uI"
      },
      "source": [
        "## Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS6w0B3nP8Zl"
      },
      "source": [
        "### ***ResNet50***\n",
        "<div>\n",
        "    <img src=\"https://s3.ap-southeast-1.amazonaws.com/datawow/uploader/blogs/1*yy6Bbnp38MhcfDQbzOGf4A.png\" width=\"800px\"/>\n",
        "    <img src=\"https://user-images.githubusercontent.com/52006798/116693634-e07f7400-a9c6-11eb-96c4-1a8ebe2ec816.png\" width=\"800px\"/>\n",
        "</div>\n",
        "<!-- ![image](https://s3.ap-southeast-1.amazonaws.com/datawow/uploader/blogs/1*yy6Bbnp38MhcfDQbzOGf4A.png) -->\n",
        "<!-- ![ResNet50](https://user-images.githubusercontent.com/52006798/116693634-e07f7400-a9c6-11eb-96c4-1a8ebe2ec816.png) -->\n",
        "`ResNet50` is a CNN that contains 50 layers.  \n",
        "A `ResNet` architecture is based on the cerebral cortex in a human's brain. A residual neural network is constructed in a way that allows it to skip connections or jump over some layers. It was first being used back in 2012 by `AlexNet` which had 8 layers deep.  \n",
        "ResNet networks were originated designed for image recognition, but are being used today for various types of tasks.  \n",
        "The major disadvantage of an ultra-deep neural network is the problem of vanishing / exploding gradients which leads to an increase in the training error.  \n",
        "\n",
        "<div>\n",
        "    <img src=\"https://iq.opengenus.org/content/images/2020/03/Screenshot-from-2020-03-20-15-26-38.png\" width=\"300px\"/>\n",
        "</div>\n",
        "<!-- ![image](https://iq.opengenus.org/content/images/2020/03/Screenshot-from-2020-03-20-15-26-38.png)   -->\n",
        "To solve this problem, this architecture implemented shortcut connections that bypass some layers. These shortcuts help in bypassing layers that lead to vanishing gradients, and as they are only a bypass, they do not add parameters to the architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5V-bHlrP-Wl"
      },
      "source": [
        "### ***EfficientNet-B3***\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://1.bp.blogspot.com/-DjZT_TLYZok/XO3BYqpxCJI/AAAAAAAAEKM/BvV53klXaTUuQHCkOXZZGywRMdU9v9T_wCLcBGAs/s1600/image2.png\" width=\"800px\"/>\n",
        "    <img src=\"https://cdn-images-1.medium.com/max/1024/1*8oE4jOMfOXeEzgsHjSB5ww.png\" width=\"800px\"/>\n",
        "</div>\n",
        "\n",
        "<!-- ![image](https://1.bp.blogspot.com/-DjZT_TLYZok/XO3BYqpxCJI/AAAAAAAAEKM/BvV53klXaTUuQHCkOXZZGywRMdU9v9T_wCLcBGAs/s1600/image2.png) -->\n",
        "<!-- ![image](https://cdn-images-1.medium.com/max/1024/1*8oE4jOMfOXeEzgsHjSB5ww.png) -->\n",
        "\n",
        "`EfficientNet` is a CNN architecture that was presented by the `Google AI` blog. These architectures are commonly used at a fixed resources budget, and can easily be scaled up for better performance if more resources become available.  \n",
        "  \n",
        "\n",
        "<div>\n",
        "    <img src=\"https://user-images.githubusercontent.com/52006798/116697418-d57b1280-a9cb-11eb-9959-23a00a71a613.png\" width=\"600px\"/>\n",
        "</div>\n",
        "<!-- ![0_FJos7uXvl-uLDpSQ](https://user-images.githubusercontent.com/52006798/116697418-d57b1280-a9cb-11eb-9959-23a00a71a613.png)   -->\n",
        "In the image above (a) is the baseline network, and (b), (c), (d) are scaling up networks that increase one dimension of the network: width, depth, or resolution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsD4OQZlQAh5"
      },
      "source": [
        "### ***Inception-V4***\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://miro.medium.com/max/1732/1*HJ3CNNGz6v76H38s7-OTSA.png\" width=\"800px\"/>\n",
        "</div>\n",
        "<!-- ![](https://miro.medium.com/max/1732/1*HJ3CNNGz6v76H38s7-OTSA.png) -->\n",
        "\n",
        "`Inception-V4` based on `Inception` architecture family which was presented by `Google` as well, envolved `GoogleLeNet`, and merged with `ResNet`.\n",
        "Inception-V4 is a simplifying of the baseline architecture and using more inception modules than the other inception architectures.\n",
        "\n",
        "A better overview of the network:  \n",
        "<div>\n",
        "    <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200502220903/Inception-V4.PNG\" width=\"800px\"/>\n",
        "</div>\n",
        "<!-- ![](https://media.geeksforgeeks.org/wp-content/uploads/20200502220903/Inception-V4.PNG)   -->\n",
        "And the inception modules are:  \n",
        "![](https://media.geeksforgeeks.org/wp-content/uploads/20200505194051/InceptionV4-Inception-block.jpeg)  \n",
        "The principle of the inception modules is to reduce computational expenses. Instead of stacking multiple kernel filters, ordering them to operate on the same level and combine their results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qbEiaOoQCe9"
      },
      "source": [
        "### ***MobileNetV3***\n",
        "\n",
        "![](https://miro.medium.com/max/640/1*mQZkmE-13Ews0sD9EQjkiw.png)\n",
        "\n",
        "`MobileNet` is an architecture family that is tuned to mobile phone CPUs, it was first presented in 2017, and lead to a new section of deep learning research for models that can run on embedded systems.  \n",
        "The MobileNet architecture uses depthwise separable convolutions which separate the dimensions of a filter, which leads to a reduction in the number of parameters. In the end, a `1X1` filter is being used to cover the depth dimension.  \n",
        "<div>\n",
        "    <img src=\"https://miro.medium.com/max/2400/1*OrBhgRQsy1y9Ikc5QkU0UA.png\" width=\"500px\"/>\n",
        "</div>\n",
        "<!-- ![](https://miro.medium.com/max/2400/1*OrBhgRQsy1y9Ikc5QkU0UA.png) -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yNH2PmgFmmX"
      },
      "source": [
        "# Lab Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GohVbrAhOm-N"
      },
      "source": [
        "## Students Details:\n",
        "* Guy Kabiri 312252224\n",
        "* Tomer Dwek 313229486\n",
        "* Tal Goldengoren 207042573\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Keu4XBv5Frmq"
      },
      "source": [
        "## Imports\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8JP77bcFx9n"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import drive\n",
        "import os \n",
        "import zipfile\n",
        "import glob\n",
        "import albumentations as A"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrBxppj0HAHQ",
        "outputId": "a25bca3e-a9e1-4286-cce0-c771537310c2"
      },
      "source": [
        "!pip install torchmetrics timm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.4.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.8.1+cu101)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (20.9)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.9.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (2.4.7)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTPbQI5vHA-B"
      },
      "source": [
        "import torchmetrics "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jdld0Qs5wwA",
        "outputId": "51a80fb8-b671-4c43-e580-495e712e193b"
      },
      "source": [
        "!pip install -U git+https://github.com/albu/albumentations > /dev/null"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-kjkkvox9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dKswQTk5ymD"
      },
      "source": [
        "from albumentations.pytorch import ToTensorV2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW5t9xHEx39R",
        "outputId": "96be7f68-7420-47a9-c946-184bb5af4e27"
      },
      "source": [
        "pip install timm"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.4.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.9.1+cu101)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.8.1+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUFPrXkkx_k1"
      },
      "source": [
        "import timm"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45CWOLcOy6MI"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard \n",
        "# %reload_ext tensorboard"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut3IyaDUxp5x"
      },
      "source": [
        "## General Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVnL47Kovjy5"
      },
      "source": [
        "SEED = 0\n",
        "np.random.seed(SEED)\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "NUM_CLASSES = 3\n",
        "experiment = 0"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfdv5edAzTaH"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVem2dCa3Nfa"
      },
      "source": [
        "def norm(img):\n",
        "    img -= img.min()\n",
        "    img /= img.max()\n",
        "    return img"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MytKyxltFpDP"
      },
      "source": [
        "## Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOgukH1qGxGA",
        "outputId": "a3470d4f-aa28-4980-89c2-e02419f13c43"
      },
      "source": [
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oughKwQF3iP"
      },
      "source": [
        "GOOGLE_DRIVE_PATH_AFTER_MY_DRIVE = 'Colab Notebooks/Computer Vision/Lab 8'\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MY_DRIVE)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UlPopXQF32e"
      },
      "source": [
        "archive_path = os.path.join(GOOGLE_DRIVE_PATH, 'archive.zip')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybYD_4NKGX-7"
      },
      "source": [
        "dir_to_extract = ''"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yKOrjvUGHPu"
      },
      "source": [
        "with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dir_to_extract)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JBPJeSFsv5o"
      },
      "source": [
        "### Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwXKsNCjDwHL"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, paths, transform=None, train=True, size=224):\n",
        "        self.paths = paths\n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "        self.size = size\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        label = p.split(\"/\")[-2].split(\"_\")[-1]\n",
        "        image = cv2.imread(p)\n",
        "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)['image']\n",
        "\n",
        "        return image, int(label) - 1 # label count starts from zero not 1 "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir4mfy2RwcxA"
      },
      "source": [
        "### Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWidjMYFtwG2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d5f9684-1be4-413b-88c4-4693e323d076"
      },
      "source": [
        "train_path = 'kaggle/train/train'\n",
        "\n",
        "images  = [glob.glob(os.path.join(train_path, d, \"*.*\")) for d in os.listdir(train_path)]\n",
        "train_paths = np.hstack(images)\n",
        "print(f'In this train set we have got a total of {len(train_paths)}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In this train set we have got a total of 1481\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyZai7al6COi"
      },
      "source": [
        "transform = A.Compose([\n",
        "    ToTensorV2()\n",
        "])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQbvpODUwqAr"
      },
      "source": [
        "trainset    = MyDataset(train_paths, transform)\n",
        "# validset    = CervicalCancerDataset(x_valid,    y_valid,    transforms=None)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLJp_lmbwHKL"
      },
      "source": [
        "train_loader  = DataLoader(dataset=trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# valid_loader  = DataLoader(dataset=validset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plUPco9SE7sB"
      },
      "source": [
        "# x,y = next(iter(train_loader))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbCaR68h1Waw"
      },
      "source": [
        "# dataloaders = { 'train'  : train_loader,\n",
        "#                 'val'    : valid_loader }\n",
        "               \n",
        "# dataset_sizes = { 'train'   : len(trainset),\n",
        "#                   'val'     : len(validset) }"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaRJinlt8VTp"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7HRdteP8b5o"
      },
      "source": [
        "### Effiecent Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlzA8C2E8W91"
      },
      "source": [
        "model = timm.create_model('efficientnet_b3', pretrained=True, num_classes=NUM_CLASSES)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkX1oz9-8e8b",
        "outputId": "3fb249fb-0431-463b-80e4-ece0c562390c"
      },
      "source": [
        "model.classifier"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=1536, out_features=3, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7BvMcji8g1Z"
      },
      "source": [
        "### ResNet 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONVkmzGa8jqo"
      },
      "source": [
        "model_resnet = models.resnet50(pretrained=True)\n",
        "in_features = model_resnet.fc.in_features\n",
        "model_resnet.fc = torch.nn.Linear(in_features, 3)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF8NNLzu8n71"
      },
      "source": [
        "### Inception V4\n",
        "\n",
        "an error has been insereted itentionaly, \n",
        "find the correct amount of features in the implementations: \n",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/inceptionv4.py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kVb5erM8rFk"
      },
      "source": [
        "model_inception_v4 = timm.create_model('inception_v4', pretrained=True, num_classes=NUM_CLASSES)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW9-F_xC8y1t"
      },
      "source": [
        "### MobileNet V3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0naV5Vc83ve"
      },
      "source": [
        "model_mobile_net3 = models.mobilenet_v3_large(pretrained=True)\n",
        "# tmp = models.mobilenet_v3_large(pretrained=False,num_classes=3)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj9C9Bip84nE",
        "outputId": "58fa5891-03af-4e22-cf06-afd41a1c5c01"
      },
      "source": [
        "model_mobile_net3.classifier"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=960, out_features=1280, bias=True)\n",
              "  (1): Hardswish()\n",
              "  (2): Dropout(p=0.2, inplace=True)\n",
              "  (3): Linear(in_features=1280, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxdDlogB85TO"
      },
      "source": [
        "model_mobile_net3.classifier[3] = torch.nn.Linear(in_features=1280, out_features=3, bias=True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbKr9XwN9BYb"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPMDEnQB9GBV"
      },
      "source": [
        "experiment += 1\n",
        "run = 0 \n",
        "writer = SummaryWriter(f'runs/run{experiment}')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "AX-jZYeO9GrQ",
        "outputId": "81fb7323-5171-49e3-f5d9-2cc714b61372"
      },
      "source": [
        "%tensorboard --logdir=runs "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 1251), started 0:05:04 ago. (Use '!kill 1251' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2rqdXZH9DMu"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFxluXUU9I0O"
      },
      "source": [
        "def save_model(model, optimizer, epoch, loss, path):\n",
        "  torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, path)\n",
        "  \n",
        "def load_model(path, model, optimizer):\n",
        "    checkpoint = torch.load(PATH)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    return epoch, loss"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U254rOHj1Hhs"
      },
      "source": [
        "### Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJQfMntHycg6"
      },
      "source": [
        "DEBUG = False \n",
        "\n",
        "def train(train_loader, model, optimizer, epoch, criterion, writer):\n",
        "    accuracy = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    model.train()\n",
        "\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    for i, (img, target) in enumerate(train_loader):\n",
        "        img = img.float()    \n",
        "        target = target.long()\n",
        "\n",
        "        img.to(device)\n",
        "        target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(img.float())\n",
        "\n",
        "        if DEBUG:\n",
        "            print('output',output.detach().cpu().numpy(),\n",
        "                    'shape', output.detach().cpu().numpy().shape )\n",
        "\n",
        "            print('target',target.detach().cpu().numpy(),\n",
        "                    'shape', target.detach().cpu().numpy().shape )\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "    \n",
        "\n",
        "        pred = output.argmax(dim=1 , keepdim=True)\n",
        "\n",
        "        acc = torchmetrics.functional.accuracy(pred, target)\n",
        "\n",
        "        print('batch: {} of {}, acc: {}, loss: {}'.format(i, num_batches, acc, loss.item()))\n",
        "\n",
        "        losses.update(loss.item(), img.size(0))\n",
        "        accuracy.update(acc, img.size(0))\n",
        "    writer.add_scalar(' average training loss', losses.avg,  i)\n",
        "    writer.add_scalar(' average training accuracy', accuracy.avg,  i)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZjIULVL9uMk"
      },
      "source": [
        "### Criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbIMoL649tzB"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXGUs5js9w7h"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3PYZpazJ-Vw"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBCswEENA1GY"
      },
      "source": [
        "epoch = 2"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf0-nzA0H9qb",
        "outputId": "a3a6e676-741c-4f4f-b81e-2f36f4f4f44a"
      },
      "source": [
        "train(train_loader, model_mobile_net3, optimizer, epoch, criterion, writer)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc: 0.375, loss: 1.1496459245681763\n",
            "acc: 0.375, loss: 1.0966687202453613\n",
            "acc: 0.375, loss: 1.081206202507019\n",
            "acc: 0.5, loss: 1.0538731813430786\n",
            "acc: 0.25, loss: 1.2350460290908813\n",
            "acc: 0.5, loss: 1.0806492567062378\n",
            "acc: 0.5, loss: 1.0626384019851685\n",
            "acc: 0.375, loss: 1.2115782499313354\n",
            "acc: 0.375, loss: 1.2420721054077148\n",
            "acc: 0.25, loss: 1.1051928997039795\n",
            "acc: 0.25, loss: 1.1576275825500488\n",
            "acc: 0.5, loss: 1.09270441532135\n",
            "acc: 0.125, loss: 1.14207124710083\n",
            "acc: 0.375, loss: 0.9917047619819641\n",
            "acc: 0.25, loss: 1.0952184200286865\n",
            "acc: 0.5, loss: 1.0699925422668457\n",
            "acc: 0.5, loss: 1.084350824356079\n",
            "acc: 0.25, loss: 1.1820173263549805\n",
            "acc: 0.25, loss: 1.1858197450637817\n",
            "acc: 0.625, loss: 1.0850348472595215\n",
            "acc: 0.375, loss: 1.064165472984314\n",
            "acc: 0.25, loss: 1.1690435409545898\n",
            "acc: 0.125, loss: 1.2379202842712402\n",
            "acc: 0.375, loss: 1.0347954034805298\n",
            "acc: 0.625, loss: 1.1423991918563843\n",
            "acc: 0.25, loss: 1.1819758415222168\n",
            "acc: 0.625, loss: 1.092038869857788\n",
            "acc: 0.375, loss: 1.0776218175888062\n",
            "acc: 0.375, loss: 1.0827603340148926\n",
            "acc: 0.125, loss: 1.220054030418396\n",
            "acc: 0.625, loss: 0.968622088432312\n",
            "acc: 0.375, loss: 1.0741666555404663\n",
            "acc: 0.125, loss: 1.0961025953292847\n",
            "acc: 0.375, loss: 1.118357539176941\n",
            "acc: 0.375, loss: 1.0601181983947754\n",
            "acc: 0.5, loss: 1.1591309309005737\n",
            "acc: 0.0, loss: 1.2093528509140015\n",
            "acc: 0.25, loss: 1.152822494506836\n",
            "acc: 0.125, loss: 1.1446726322174072\n",
            "acc: 0.125, loss: 1.145111083984375\n",
            "acc: 0.125, loss: 1.1971548795700073\n",
            "acc: 0.625, loss: 1.0066574811935425\n",
            "acc: 0.25, loss: 1.185469627380371\n",
            "acc: 0.0, loss: 1.168741226196289\n",
            "acc: 0.25, loss: 1.1129601001739502\n",
            "acc: 0.5, loss: 1.0155141353607178\n",
            "acc: 0.25, loss: 1.1836233139038086\n",
            "acc: 0.125, loss: 1.1376336812973022\n",
            "acc: 0.125, loss: 1.217405915260315\n",
            "acc: 0.375, loss: 1.118849754333496\n",
            "acc: 0.25, loss: 1.268707275390625\n",
            "acc: 0.5, loss: 1.0476716756820679\n",
            "acc: 0.375, loss: 1.0668542385101318\n",
            "acc: 0.375, loss: 1.1702684164047241\n",
            "acc: 0.25, loss: 1.1737140417099\n",
            "acc: 0.875, loss: 0.9037492275238037\n",
            "acc: 0.125, loss: 1.2212592363357544\n",
            "acc: 0.125, loss: 1.2044256925582886\n",
            "acc: 0.375, loss: 1.1385009288787842\n",
            "acc: 0.25, loss: 1.0462578535079956\n",
            "acc: 0.375, loss: 1.0514097213745117\n",
            "acc: 0.5, loss: 1.106705904006958\n",
            "acc: 0.25, loss: 1.240598440170288\n",
            "acc: 0.125, loss: 1.2248051166534424\n",
            "acc: 0.25, loss: 1.164661169052124\n",
            "acc: 0.625, loss: 0.9989806413650513\n",
            "acc: 0.5, loss: 1.0432744026184082\n",
            "acc: 0.375, loss: 1.110263466835022\n",
            "acc: 0.375, loss: 1.0412530899047852\n",
            "acc: 0.125, loss: 1.1781814098358154\n",
            "acc: 0.5, loss: 1.0036647319793701\n",
            "acc: 0.25, loss: 1.1610336303710938\n",
            "acc: 0.125, loss: 1.1525511741638184\n",
            "acc: 0.125, loss: 1.0883222818374634\n",
            "acc: 0.5, loss: 1.1183300018310547\n",
            "acc: 0.5, loss: 1.018929362297058\n",
            "acc: 0.375, loss: 1.2120094299316406\n",
            "acc: 0.25, loss: 1.3278956413269043\n",
            "acc: 0.0, loss: 1.1162809133529663\n",
            "acc: 0.5, loss: 1.1973810195922852\n",
            "acc: 0.75, loss: 0.9159921407699585\n",
            "acc: 0.125, loss: 1.2370141744613647\n",
            "acc: 0.625, loss: 1.015055775642395\n",
            "acc: 0.125, loss: 1.198950171470642\n",
            "acc: 0.375, loss: 1.0521509647369385\n",
            "acc: 0.5, loss: 1.0071837902069092\n",
            "acc: 0.25, loss: 1.0880595445632935\n",
            "acc: 0.25, loss: 1.0706706047058105\n",
            "acc: 0.25, loss: 1.160614252090454\n",
            "acc: 0.375, loss: 1.0561354160308838\n",
            "acc: 0.375, loss: 1.180354356765747\n",
            "acc: 0.375, loss: 1.1508581638336182\n",
            "acc: 0.25, loss: 1.1139894723892212\n",
            "acc: 0.125, loss: 1.2613368034362793\n",
            "acc: 0.25, loss: 1.2393673658370972\n",
            "acc: 0.5, loss: 0.9942730665206909\n",
            "acc: 0.25, loss: 1.2094354629516602\n",
            "acc: 0.25, loss: 1.0509628057479858\n",
            "acc: 0.25, loss: 1.145026445388794\n",
            "acc: 0.5, loss: 1.089143991470337\n",
            "acc: 0.375, loss: 1.2113066911697388\n",
            "acc: 0.375, loss: 1.1355682611465454\n",
            "acc: 0.375, loss: 1.0782192945480347\n",
            "acc: 0.25, loss: 1.076627254486084\n",
            "acc: 0.25, loss: 1.1482341289520264\n",
            "acc: 0.25, loss: 1.0745506286621094\n",
            "acc: 0.5, loss: 1.076782464981079\n",
            "acc: 0.625, loss: 1.0599138736724854\n",
            "acc: 0.25, loss: 1.169884443283081\n",
            "acc: 0.375, loss: 1.076432704925537\n",
            "acc: 0.25, loss: 1.1825065612792969\n",
            "acc: 0.25, loss: 1.2016313076019287\n",
            "acc: 0.375, loss: 1.1661852598190308\n",
            "acc: 0.25, loss: 1.1884198188781738\n",
            "acc: 0.25, loss: 1.0567001104354858\n",
            "acc: 0.125, loss: 1.0763423442840576\n",
            "acc: 0.375, loss: 1.1546961069107056\n",
            "acc: 0.375, loss: 1.1099895238876343\n",
            "acc: 0.25, loss: 1.1068811416625977\n",
            "acc: 0.5, loss: 1.09568452835083\n",
            "acc: 0.25, loss: 1.145550012588501\n",
            "acc: 0.25, loss: 1.093485713005066\n",
            "acc: 0.25, loss: 1.1180956363677979\n",
            "acc: 0.5, loss: 1.0168826580047607\n",
            "acc: 0.25, loss: 1.158560037612915\n",
            "acc: 0.375, loss: 1.132308006286621\n",
            "acc: 0.125, loss: 1.1629995107650757\n",
            "acc: 0.125, loss: 1.2175452709197998\n",
            "acc: 0.25, loss: 1.0689579248428345\n",
            "acc: 0.375, loss: 1.077324628829956\n",
            "acc: 0.125, loss: 1.098244071006775\n",
            "acc: 0.25, loss: 1.1942890882492065\n",
            "acc: 0.625, loss: 1.0297400951385498\n",
            "acc: 0.375, loss: 1.1271687746047974\n",
            "acc: 0.375, loss: 1.0785568952560425\n",
            "acc: 0.5, loss: 1.2202444076538086\n",
            "acc: 0.375, loss: 1.085151195526123\n",
            "acc: 0.625, loss: 1.017699122428894\n",
            "acc: 0.5, loss: 1.0169858932495117\n",
            "acc: 0.375, loss: 0.9912608861923218\n",
            "acc: 0.125, loss: 1.1926448345184326\n",
            "acc: 0.375, loss: 1.1081435680389404\n",
            "acc: 0.375, loss: 1.0933899879455566\n",
            "acc: 0.375, loss: 1.0631709098815918\n",
            "acc: 0.125, loss: 1.1333632469177246\n",
            "acc: 0.5, loss: 1.027238368988037\n",
            "acc: 0.5, loss: 0.9875131845474243\n",
            "acc: 0.625, loss: 1.0710222721099854\n",
            "acc: 0.375, loss: 1.0946978330612183\n",
            "acc: 0.25, loss: 1.0604168176651\n",
            "acc: 0.375, loss: 1.034889817237854\n",
            "acc: 0.375, loss: 1.1654025316238403\n",
            "acc: 0.375, loss: 1.109266996383667\n",
            "acc: 0.5, loss: 1.115551471710205\n",
            "acc: 0.375, loss: 1.171836256980896\n",
            "acc: 0.25, loss: 1.2036539316177368\n",
            "acc: 0.625, loss: 1.0885660648345947\n",
            "acc: 0.375, loss: 1.1280630826950073\n",
            "acc: 0.375, loss: 1.0528401136398315\n",
            "acc: 0.125, loss: 1.135713815689087\n",
            "acc: 0.5, loss: 1.0306503772735596\n",
            "acc: 0.25, loss: 1.1520705223083496\n",
            "acc: 0.125, loss: 1.0939826965332031\n",
            "acc: 0.375, loss: 1.0936440229415894\n",
            "acc: 0.0, loss: 1.2629271745681763\n",
            "acc: 0.25, loss: 1.1673011779785156\n",
            "acc: 0.625, loss: 0.9921578764915466\n",
            "acc: 0.5, loss: 1.0793142318725586\n",
            "acc: 0.375, loss: 1.0473599433898926\n",
            "acc: 0.5, loss: 1.018442988395691\n",
            "acc: 0.25, loss: 1.164304494857788\n",
            "acc: 0.5, loss: 1.000333547592163\n",
            "acc: 0.25, loss: 1.1978094577789307\n",
            "acc: 0.25, loss: 1.2190675735473633\n",
            "acc: 0.625, loss: 1.0094938278198242\n",
            "acc: 0.5, loss: 1.0950418710708618\n",
            "acc: 0.375, loss: 1.0973517894744873\n",
            "acc: 0.375, loss: 1.0942119359970093\n",
            "acc: 0.25, loss: 1.140951156616211\n",
            "acc: 0.625, loss: 1.0773144960403442\n",
            "acc: 0.25, loss: 1.143946886062622\n",
            "acc: 0.375, loss: 1.092106819152832\n",
            "acc: 0.375, loss: 0.9824975728988647\n",
            "acc: 0.25, loss: 1.138792872428894\n",
            "acc: 0.5, loss: 1.0958349704742432\n",
            "acc: 0.0, loss: 1.3273944854736328\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}